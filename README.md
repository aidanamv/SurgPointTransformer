# SurgPointTransformer

A transformer-based pipeline for vertebra shape completion using RGB-D imaging. This method enables 3D spine reconstruction in surgical settings without ionizing radiation by leveraging sparse RGB-D data and transformer architectures.

## ğŸ”¬ Overview

SurgPointTransformer is a novel approach to reconstruct complete vertebral anatomy from partial RGB-D data captured intraoperatively. The pipeline:

* Segments visible spinal regions from RGB-D images.
* Performs vertebra-wise shape completion using a transformer-based network.
* Outputs high-quality 3D mesh reconstructions without relying on radiation-based imaging like CT or fluoroscopy.

> ğŸ“Œ This project was developed at the University Hospital Balgrist, Zurich.

## ğŸ“· Motivation

Intraoperative imaging methods like CT and fluoroscopy expose patients and clinicians to radiation. SurgPointTransformer addresses this by enabling radiation-free 3D shape reconstruction using RGB-D cameras, inspired by how surgeons mentally reconstruct anatomy from partial exposure.

## ğŸ§  Key Features

* ğŸ§  Transformer-based shape completion (built on AdaPointr)
* ğŸ“¦ Integrated vertebra segmentation using YOLOv8 + SAM + PointNet++
* ğŸ”„ No registration to preoperative data required
* ğŸ’» Real-time capable pipeline (GPU-accelerated)
* ğŸ“Š Evaluated on SpineDepth dataset (ex vivo RGB-D surgeries)

## ğŸ—ï¸ Pipeline

```
RGB-D Images â†’ Spine Localization â†’ Vertebra Segmentation â†’ Shape Completion â†’ 3D Mesh
```

### Components:

* **Localization**: YOLOv8 trained on RGB images
* **Segmentation**: SAM + PointNet++ for vertebra-level point clouds
* **Shape Completion**: SurgPointTransformer using geometric transformer blocks
* **3D Reconstruction**: Poisson surface reconstruction for mesh generation

## ğŸ“Š Results

| Metric                 | SurgPointTransformer | VRCNet (Baseline) |
| ---------------------- | -------------------- | ----------------- |
| Chamfer Distance (mm)  | **5.39**             | 6.17              |
| F-score                | 0.85                 | **0.86**          |
| Earth Moverâ€™s Distance | **11.00**            | 20.00             |
| SNR (dB)               | **22.90**            | 22.89             |

> ğŸ¯ Significantly better shape fidelity and robustness to noise and occlusions compared to state-of-the-art.

## ğŸ—ƒï¸ Dataset

We used:

* **SpineDepth Dataset**: 9 ex vivo spinal surgery cases with RGB-D recordings and paired CT ground-truths
* **CTSpine1K (synthetic)**: For ablation studies on noise and input-to-output ratios

## ğŸ§ª Ablation Studies

* Evaluated on noise levels (low, medium, high)
* Varying visibility of vertebral anatomy (10% to 40%)
* Demonstrated robustness to partial input and sensor noise

## âš™ï¸ Installation

```bash
git clone https://github.com/yourusername/SurgPointTransformer.git
cd SurgPointTransformer
pip install -r requirements.txt
```

## ğŸš€ Training & Inference

```bash
# Train YOLOv8 for localization
python train_yolo.py \
  --model yolov10n.yaml \
  --data dataset.yaml \
  --epochs 50 \
  --imgsz 640 \
  --batch 16 \
  --device 0 \
  --name spine_detection_v10n


# visualize YOLO results with ground-truth stls
python visualize_yolo_stl.py \
  --model ./weights/best.pt \
  --specimen 2 \
  --rgb_dir ./data/fold_2/test/images \
  --depth_root ./data/fold_2/test/depth  \
  --stl_root ./data/fold_2/test/stls \
  --calib_dir ./data/fold_2/calib/SN10027879.conf
  
  
# evaluate YOLO results
python evaluate_yolo.py \
  --model ./weights/best.pt \
  --data dataset.yaml



# Train PointNet++ for segmentation
python train_segmentation.py \
  --dataset ./data/PointNet_data \
  --outf ./checkpoints/ \
  --fold 2 \
  --batchSize 32 \
  --nepoch 25 \
  --channels 3 \
  --feature_transform


# Run inference Example of PointNet++ for segmentation
python evaluate_segmentation.py \
  --dataset_root ./data/PointNet_data \
  --stl_root ./data/stls \
  --checkpoints_dir ./checkpoints \
  --output_dir ./results \
  --channels 3 \
  --folds 2 


# Train SurgPointTransformer
python train_transformer.py

# Run inference
python inference.py --input path/to/rgbd
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/                # SpineDepth RGB-D data & GT meshes
â”œâ”€â”€ models/              # YOLOv8, PointNet++, Transformer
â”œâ”€â”€ scripts/             # Training/inference scripts
â”œâ”€â”€ utils/               # Metrics, visualizations, point cloud tools
â””â”€â”€ README.md
```



## ğŸ“œ Citation

```bibtex
@article{massalimova2025surgpointtransformer,
  title={SurgPointTransformer: transformer-based vertebra shape completion using RGB-D imaging},
  author={Massalimova, Aidana and Liebmann, Florentin and Jecklin, Sascha and Carrillo, Fabio and Farshad, Mazda and FÃ¼rnstahl, Philipp},
  journal={Computer Assisted Surgery},
  volume={30},
  number={1},
  pages={2511126},
  year={2025},
  publisher={Taylor & Francis},
  doi={10.1080/24699322.2025.2511126}
}
```

## ğŸ›¡ï¸ License

This project is licensed under the **Creative Commons Attribution-NonCommercial (CC BY-NC 4.0)** license.

