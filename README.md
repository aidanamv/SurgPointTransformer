# SurgPointTransformer

A transformer-based pipeline for vertebra shape completion using RGB-D imaging. This method enables 3D spine reconstruction in surgical settings without ionizing radiation by leveraging sparse RGB-D data and transformer architectures.

## ğŸ”¬ Overview

SurgPointTransformer is a novel approach to reconstruct complete vertebral anatomy from partial RGB-D data captured intraoperatively. The pipeline:

* Segments visible spinal regions from RGB-D images.
* Performs vertebra-wise shape completion using a transformer-based network.
* Outputs high-quality 3D mesh reconstructions without relying on radiation-based imaging like CT or fluoroscopy.

> ğŸ“Œ This project was developed at the University Hospital Balgrist, Zurich.

## ğŸ“· Motivation

Intraoperative imaging methods like CT and fluoroscopy expose patients and clinicians to radiation. SurgPointTransformer addresses this by enabling radiation-free 3D shape reconstruction using RGB-D cameras, inspired by how surgeons mentally reconstruct anatomy from partial exposure.

## ğŸ§  Key Features

* ğŸ§  Transformer-based shape completion (built on AdaPointr)
* ğŸ“¦ Integrated vertebra segmentation using YOLOv8 + SAM + PointNet++
* ğŸ”„ No registration to preoperative data required
* ğŸ’» Real-time capable pipeline (GPU-accelerated)
* ğŸ“Š Evaluated on SpineDepth dataset (ex vivo RGB-D surgeries)

## ğŸ—ï¸ Pipeline

```
RGB-D Images â†’ Spine Localization â†’ Vertebra Segmentation â†’ Shape Completion â†’ 3D Mesh
```

### Components:

* **Localization**: YOLOv8 trained on RGB images
* **Segmentation**: SAM + PointNet++ for vertebra-level point clouds
* **Shape Completion**: SurgPointTransformer using geometric transformer blocks
* **3D Reconstruction**: Poisson surface reconstruction for mesh generation

## ğŸ“Š Results

| Metric                 | SurgPointTransformer | VRCNet (Baseline) |
| ---------------------- | -------------------- | ----------------- |
| Chamfer Distance (mm)  | **5.39**             | 6.17              |
| F-score                | 0.85                 | **0.86**          |
| Earth Moverâ€™s Distance | **11.00**            | 20.00             |
| SNR (dB)               | **22.90**            | 22.89             |

> ğŸ¯ Significantly better shape fidelity and robustness to noise and occlusions compared to state-of-the-art.

## ğŸ—ƒï¸ Dataset

We used:

* **SpineDepth Dataset**: 9 ex vivo spinal surgery cases with RGB-D recordings and paired CT ground-truths
* **CTSpine1K (synthetic)**: For ablation studies on noise and input-to-output ratios

## ğŸ§ª Ablation Studies

* Evaluated on noise levels (low, medium, high)
* Varying visibility of vertebral anatomy (10% to 40%)
* Demonstrated robustness to partial input and sensor noise

## âš™ï¸ Installation

```bash
git clone https://github.com/yourusername/SurgPointTransformer.git
cd SurgPointTransformer
pip install -r requirements.txt
```

## ğŸš€ Training & Inference

### ğŸŸ¢ **Train YOLOv8 / v10 for Localization**

```bash
python YOLO/train_yolo.py \
  --model <path/to/yolo_model.yaml> \
  --data <path/to/dataset.yaml> \
  --epochs <num_epochs> \
  --imgsz <image_size> \
  --batch <batch_size> \
  --device <cuda_device_id> \
  --name <run_name>
```

---

### ğŸŸ¢ **Visualize YOLO Results with STL Meshes**

```bash
python YOLO/visualize_yolo_stl.py \
  --model <path/to/best.pt> \
  --specimen <specimen_id> \
  --rgb_dir <path/to/rgb/images> \
  --depth_root <path/to/depth/maps> \
  --stl_root <path/to/stl/meshes> \
  --calib_dir <path/to/camera_calib.conf>
```

---

### ğŸŸ¢ **Evaluate YOLO Predictions**

```bash
python YOLO/evaluate_yolo.py \
  --model <path/to/best.pt> \
  --data <path/to/dataset.yaml>
```

---

### ğŸŸ¢ **Train PointNet++ for Segmentation**

```bash
python PointNet/train_segmentation.py \
  --dataset <path/to/PointNet_data> \
  --outf <output_dir_for_checkpoints> \
  --fold <fold_number> \
  --batchSize <batch_size> \
  --nepoch <num_epochs> \
  --channels <num_input_channels> \
  --feature_transform
```

---

### ğŸŸ¢ **Evaluate PointNet++ Segmentation**

```bash
python PointNet/evaluate_segmentation.py \
  --dataset_root <path/to/PointNet_data> \
  --stl_root <path/to/stl_root> \
  --checkpoints_dir <path/to/checkpoints_dir> \
  --output_dir <path/to/save_results> \
  --channels <num_input_channels> \
  --folds <fold_number>
```

---

### ğŸŸ¢ **Test VRCNet**

```bash
python VRCNet/test_vrcnet.py \
  --config <path/to/vrcnet_config.yaml> \
  --fold <fold_number> \
  --model <model_file_name_or_tag> \
  --checkpoints <path/to/ckpt-best.pth> \
  --dataset <path/to/data_dir>
```

---

### ğŸŸ¢ **Train VRCNet**

```bash
python VRCNet/train_vrcnet.py \
  --config <path/to/train_config.yaml> \
  --dataset <path/to/data_dir> \
  --fold <fold_number>
```

---

### ğŸŸ¢ **Train SurgPointTransformer**

```bash
python PoinTr/main.py \
  --config <path/to/PointTransformer_config.yaml>
```

---

### ğŸŸ¢ **Test SurgPointTransformer**

```bash
python PoinTr/main.py \
  --test \
  --config <path/to/PointTransformer_config.yaml> \
  --ckpts <path/to/ckpt-best.pth>
```

---




## ğŸ“ Project Structure

```
â”œâ”€â”€ YOLO/                            # YOLOv8/YOLOv10 training, inference, evaluation
â”‚   â”œâ”€â”€ train_yolo.py                # Train YOLO for localization
â”‚   â”œâ”€â”€ visualize_yolo_stl.py       # Visualize YOLO boxes + STL meshes
â”‚   â”œâ”€â”€ evaluate_yolo.py            # Evaluate detection results
    â”œâ”€â”€ dataset.yaml                 #config file for training 
â”‚   â””â”€â”€ weights/                    # Trained YOLO models (.pt)
â”‚
â”œâ”€â”€ PointNet/                        # PointNet++ segmentation
â”‚   â”œâ”€â”€ train_segmentation.py       # Train segmentation model
â”‚   â”œâ”€â”€ evaluate_segmentation.py    # Inference + metric evaluation
â”‚   â”œâ”€â”€ model.py                    # PointNet++ model definition
â”‚   â”œâ”€â”€ utils.py                    # utils for loss, metrics, model
â”‚   â”œâ”€â”€ dataloader.py               # Custom dataset loader
â”‚   â””â”€â”€ checkpoints/                # Saved model checkpoints
â”‚
â”œâ”€â”€ VRCNet/                          # VRCNet shape completion baseline
â”‚   â”œâ”€â”€ test_vrcnet.py              # Evaluate VRCNet
â”‚   â”œâ”€â”€ train_vrcnet.py             # Train VRCNet
â”‚   â”œâ”€â”€ dataset.py                  # dataloader
â”‚   â””â”€â”€ cfgs/                       # VRCNet YAML configs
â”‚
â”œâ”€â”€ PoinTr/                          # SurgPointTransformer
â”‚   â”œâ”€â”€ main.py                     # Train/test entrypoint
â”‚   â”œâ”€â”€ tools/                      # Utils for loss, model, metrics
â”‚   â”œâ”€â”€ models/                     # AdaPoinTr transformer models
â”‚   â”œâ”€â”€ cfgs/                       # Training YAML configurations
â”‚   â””â”€â”€ experiments/                # Checkpoints and logs
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md

```



## ğŸ“œ Citation

```bibtex
@article{massalimova2025surgpointtransformer,
  title={SurgPointTransformer: transformer-based vertebra shape completion using RGB-D imaging},
  author={Massalimova, Aidana and Liebmann, Florentin and Jecklin, Sascha and Carrillo, Fabio and Farshad, Mazda and FÃ¼rnstahl, Philipp},
  journal={Computer Assisted Surgery},
  volume={30},
  number={1},
  pages={2511126},
  year={2025},
  publisher={Taylor & Francis},
  doi={10.1080/24699322.2025.2511126}
}
```

## ğŸ›¡ï¸ License

This project is licensed under the **Creative Commons Attribution-NonCommercial (CC BY-NC 4.0)** license.

